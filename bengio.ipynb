{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Probabilistic Language Model (Bengio et al., 2003) - Replica\n",
        "\n",
        "This notebook implements a replica of the Neural Probabilistic Language Model with MLflow integration for experiment tracking.\n",
        "\n",
        "## MLflow Persistence Setup\n",
        "We store MLflow results on Google Drive to ensure persistence across Colab sessions.\n",
        "\n",
        "**Advantages of this approach:**\n",
        "- Simple to implement (just mount Drive and set tracking URI)\n",
        "- No external server required\n",
        "- Data persists across Colab sessions\n",
        "- Easy to share results with team members via Drive sharing"
      ],
      "metadata": {
        "id": "intro_md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Mount Google Drive and setup paths\n",
        "import sys\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print('Not running in Google Colab. Using local paths.')\n",
        "\n",
        "# Project paths\n",
        "if IN_COLAB:\n",
        "    project_path = '/content/drive/MyDrive/Deep_learning_papers/Neural_probabilistic_laguage_model'\n",
        "    mlflow_tracking_path = '/content/drive/MyDrive/Deep_learning_papers/Neural_probabilistic_laguage_model/mlruns'\n",
        "else:\n",
        "    project_path = '.'\n",
        "    mlflow_tracking_path = './mlruns'\n",
        "\n",
        "# Create mlruns directory if it doesn't exist\n",
        "os.makedirs(mlflow_tracking_path, exist_ok=True)\n",
        "\n",
        "if project_path not in sys.path:\n",
        "    sys.path.insert(0, project_path)\n",
        "\n",
        "from utils.data_preparator import *"
      ],
      "metadata": {
        "id": "drive_mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install and setup MLflow\n",
        "try:\n",
        "    import mlflow\n",
        "except ImportError:\n",
        "    !pip install mlflow -q\n",
        "    import mlflow\n",
        "\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Set the tracking URI to Google Drive path for persistence\n",
        "mlflow.set_tracking_uri(f'file://{mlflow_tracking_path}')\n",
        "\n",
        "# Create or get experiment\n",
        "experiment_name = 'bengio_language_model'\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f'MLflow tracking URI: {mlflow.get_tracking_uri()}')\n",
        "print(f'MLflow experiment: {experiment_name}')"
      ],
      "metadata": {
        "id": "mlflow_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import dependencies\n",
        "import nltk\n",
        "nltk.download('brown', quiet=True)\n",
        "from nltk.corpus import brown\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import Counter\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Device setup (automatic CUDA/CPU detection)\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {
        "id": "device_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Model layers (Andrej Karpathy style)\n",
        "class Linear:\n",
        "    def __init__(self, fan_in, fan_out, bias=True, device=None):\n",
        "        self.weight = (torch.randn((fan_in, fan_out)) / fan_in**0.5).to(device)\n",
        "        self.bias = torch.zeros(fan_out).to(device) if bias else None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "\n",
        "class Embeddings:\n",
        "    def __init__(self, num_embeddings, embedding_dim, device=None):\n",
        "        self.weight = torch.randn((num_embeddings, embedding_dim)).to(device)\n",
        "\n",
        "    def __call__(self, IX):\n",
        "        self.out = self.weight[IX]\n",
        "        # Flatten: [Batch, context_window, embedding_dim] -> [Batch, context_window * embedding_dim]\n",
        "        self.out = self.out.view(self.out.shape[0], -1)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight]\n",
        "\n",
        "\n",
        "class Sequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        self.out = x\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]"
      ],
      "metadata": {
        "id": "model_layers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Model factory function\n",
        "def create_model(vocab_size, n_emb, n_hidden, context_window, device):\n",
        "    \"\"\"Create a Bengio-style neural language model.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        n_emb: Embedding dimension\n",
        "        n_hidden: Hidden layer dimension\n",
        "        context_window: Number of context words (block_size)\n",
        "        device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "        model: Sequential model\n",
        "        n_params: Number of trainable parameters\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embeddings(vocab_size, n_emb, device=device),\n",
        "        Linear(n_emb * context_window, n_hidden, bias=True, device=device),\n",
        "        Tanh(),\n",
        "        Linear(n_hidden, vocab_size, bias=True, device=device),\n",
        "    ])\n",
        "    \n",
        "    parameters = model.parameters()\n",
        "    n_params = sum(p.nelement() for p in parameters)\n",
        "    for p in parameters:\n",
        "        p.requires_grad = True\n",
        "    \n",
        "    return model, n_params"
      ],
      "metadata": {
        "id": "model_factory"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, X, Y, batch_size=256):\n",
        "    \"\"\"Evaluate loss on a dataset.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        X: Input tensor\n",
        "        Y: Target tensor\n",
        "        batch_size: Batch size for evaluation\n",
        "    \n",
        "    Returns:\n",
        "        loss: Mean cross-entropy loss\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        Xb = X[i:i + batch_size]\n",
        "        Yb = Y[i:i + batch_size]\n",
        "        logits = model(Xb)\n",
        "        loss = F.cross_entropy(logits, Yb)\n",
        "        losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "evaluate_loss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Main training function with MLflow logging\n",
        "def train_experiment(params):\n",
        "    \"\"\"Run a training experiment with the given parameters.\n",
        "    \n",
        "    Args:\n",
        "        params: Dictionary containing:\n",
        "            - embedding_dim: Dimension of word embeddings\n",
        "            - hidden_dim: Dimension of hidden layer\n",
        "            - learning_rate: Learning rate\n",
        "            - batch_size: Training batch size\n",
        "            - context_window: Number of context words\n",
        "            - max_steps: Maximum training steps\n",
        "            - eval_interval: Evaluate every N steps\n",
        "            - lr_decay_step: Step at which to decay learning rate (optional)\n",
        "            - seed: Random seed (optional)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including final train_loss, val_loss, and perplexity\n",
        "    \"\"\"\n",
        "    # Extract parameters with defaults\n",
        "    n_emb = params.get('embedding_dim', 60)\n",
        "    n_hidden = params.get('hidden_dim', 100)\n",
        "    learning_rate = params.get('learning_rate', 0.1)\n",
        "    batch_size = params.get('batch_size', 64)\n",
        "    context_window = params.get('context_window', 3)\n",
        "    max_steps = params.get('max_steps', 10000)\n",
        "    eval_interval = params.get('eval_interval', 1000)\n",
        "    lr_decay_step = params.get('lr_decay_step', 150000)\n",
        "    seed = params.get('seed', 42)\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    # Build dataset\n",
        "    Xtr, Xval, Xte, Ytr, Yval, Yte, vocab_size, stoi, itos = build_dataset(\n",
        "        brown.words(), context_window, device=device\n",
        "    )\n",
        "    \n",
        "    # Create model\n",
        "    model, n_params = create_model(vocab_size, n_emb, n_hidden, context_window, device)\n",
        "    parameters = model.parameters()\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Starting experiment: n_emb={n_emb}, n_hidden={n_hidden}')\n",
        "    print(f'Model parameters: {n_params:,}')\n",
        "    print(f'Vocab size: {vocab_size}')\n",
        "    print(f'{\"=\"*60}\\n')\n",
        "    \n",
        "    # Start MLflow run\n",
        "    with mlflow.start_run():\n",
        "        # Log parameters\n",
        "        mlflow.log_params({\n",
        "            'embedding_dim': n_emb,\n",
        "            'hidden_dim': n_hidden,\n",
        "            'learning_rate': learning_rate,\n",
        "            'batch_size': batch_size,\n",
        "            'context_window': context_window,\n",
        "            'max_steps': max_steps,\n",
        "            'vocab_size': vocab_size,\n",
        "            'n_params': n_params,\n",
        "            'seed': seed,\n",
        "            'device': device\n",
        "        })\n",
        "        \n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Sample batch\n",
        "            ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "            Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(Xb)\n",
        "            loss = F.cross_entropy(logits, Yb)\n",
        "            \n",
        "            # Backward pass\n",
        "            for p in parameters:\n",
        "                p.grad = None\n",
        "            loss.backward()\n",
        "            \n",
        "            # Learning rate schedule\n",
        "            lr = learning_rate if step < lr_decay_step else learning_rate * 0.1\n",
        "            \n",
        "            # Update parameters\n",
        "            for p in parameters:\n",
        "                p.data += -lr * p.grad\n",
        "            \n",
        "            # Log and evaluate\n",
        "            if step % eval_interval == 0 or step == max_steps - 1:\n",
        "                train_loss = evaluate_loss(model, Xtr, Ytr)\n",
        "                val_loss = evaluate_loss(model, Xval, Yval)\n",
        "                train_perplexity = math.exp(train_loss)\n",
        "                val_perplexity = math.exp(val_loss)\n",
        "                \n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                \n",
        "                # Log metrics to MLflow\n",
        "                mlflow.log_metrics({\n",
        "                    'train_loss': train_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                    'train_perplexity': train_perplexity,\n",
        "                    'val_perplexity': val_perplexity\n",
        "                }, step=step)\n",
        "                \n",
        "                print(f'Step {step:6d}/{max_steps}: '\n",
        "                      f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, '\n",
        "                      f'train_ppl={train_perplexity:.2f}, val_ppl={val_perplexity:.2f}')\n",
        "        \n",
        "        # Final evaluation\n",
        "        final_train_loss = train_losses[-1]\n",
        "        final_val_loss = val_losses[-1]\n",
        "        final_train_ppl = math.exp(final_train_loss)\n",
        "        final_val_ppl = math.exp(final_val_loss)\n",
        "        test_loss = evaluate_loss(model, Xte, Yte)\n",
        "        test_perplexity = math.exp(test_loss)\n",
        "        \n",
        "        # Log final metrics\n",
        "        mlflow.log_metrics({\n",
        "            'final_train_loss': final_train_loss,\n",
        "            'final_val_loss': final_val_loss,\n",
        "            'final_test_loss': test_loss,\n",
        "            'final_train_perplexity': final_train_ppl,\n",
        "            'final_val_perplexity': final_val_ppl,\n",
        "            'final_test_perplexity': test_perplexity\n",
        "        })\n",
        "        \n",
        "        print(f'\\nFinal Results:')\n",
        "        print(f'  Train Loss: {final_train_loss:.4f}, Perplexity: {final_train_ppl:.2f}')\n",
        "        print(f'  Val Loss:   {final_val_loss:.4f}, Perplexity: {final_val_ppl:.2f}')\n",
        "        print(f'  Test Loss:  {test_loss:.4f}, Perplexity: {test_perplexity:.2f}')\n",
        "        \n",
        "        # Log number of parameters as metric for easy comparison\n",
        "        mlflow.log_metric('n_params', n_params)\n",
        "        \n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "    \n",
        "    return {\n",
        "        'run_id': run_id,\n",
        "        'train_loss': final_train_loss,\n",
        "        'val_loss': final_val_loss,\n",
        "        'test_loss': test_loss,\n",
        "        'train_perplexity': final_train_ppl,\n",
        "        'val_perplexity': final_val_ppl,\n",
        "        'test_perplexity': test_perplexity,\n",
        "        'n_params': n_params,\n",
        "        'model': model,\n",
        "        'itos': itos,\n",
        "        'stoi': stoi\n",
        "    }"
      ],
      "metadata": {
        "id": "train_experiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Define experiment configurations\n",
        "# Based on Bengio et al. (2003), we test smaller configurations to reduce overfitting\n",
        "# The original paper used embedding dimensions around 30-100 and hidden layer sizes of 50-200\n",
        "\n",
        "experiments = [\n",
        "    # Small configuration (from Bengio paper)\n",
        "    {\n",
        "        'embedding_dim': 30,\n",
        "        'hidden_dim': 50,\n",
        "        'learning_rate': 0.1,\n",
        "        'batch_size': 64,\n",
        "        'context_window': 3,\n",
        "        'max_steps': 10000,\n",
        "        'eval_interval': 1000,\n",
        "    },\n",
        "    # Medium configuration (from Bengio paper)\n",
        "    {\n",
        "        'embedding_dim': 60,\n",
        "        'hidden_dim': 100,\n",
        "        'learning_rate': 0.1,\n",
        "        'batch_size': 64,\n",
        "        'context_window': 3,\n",
        "        'max_steps': 10000,\n",
        "        'eval_interval': 1000,\n",
        "    },\n",
        "    # Large configuration (overfitting baseline)\n",
        "    {\n",
        "        'embedding_dim': 150,\n",
        "        'hidden_dim': 400,\n",
        "        'learning_rate': 0.1,\n",
        "        'batch_size': 64,\n",
        "        'context_window': 3,\n",
        "        'max_steps': 10000,\n",
        "        'eval_interval': 1000,\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f'Configured {len(experiments)} experiments:')\n",
        "for i, exp in enumerate(experiments, 1):\n",
        "    print(f'  {i}. n_emb={exp[\"embedding_dim\"]}, n_hidden={exp[\"hidden_dim\"]}')"
      ],
      "metadata": {
        "id": "experiment_configs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Run all experiments\n",
        "results = []\n",
        "\n",
        "for i, params in enumerate(experiments, 1):\n",
        "    print(f'\\n{\"#\"*60}')\n",
        "    print(f'Running Experiment {i}/{len(experiments)}')\n",
        "    print(f'{\"#\"*60}')\n",
        "    \n",
        "    result = train_experiment(params)\n",
        "    result['config'] = params\n",
        "    results.append(result)\n",
        "    \n",
        "    # Clear CUDA cache between experiments\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print('All experiments completed!')\n",
        "print(f'{\"=\"*60}')"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Summary of results\n",
        "print('\\nExperiment Summary:')\n",
        "print('-' * 80)\n",
        "print(f'{\"Config\":^25} | {\"Params\":>10} | {\"Train PPL\":>10} | {\"Val PPL\":>10} | {\"Test PPL\":>10}')\n",
        "print('-' * 80)\n",
        "\n",
        "for r in results:\n",
        "    config = r['config']\n",
        "    config_str = f\"n_emb={config['embedding_dim']}, n_h={config['hidden_dim']}\"\n",
        "    print(f'{config_str:^25} | {r[\"n_params\"]:>10,} | {r[\"train_perplexity\"]:>10.2f} | {r[\"val_perplexity\"]:>10.2f} | {r[\"test_perplexity\"]:>10.2f}')\n",
        "\n",
        "print('-' * 80)\n",
        "\n",
        "# Find best model by validation perplexity\n",
        "best_result = min(results, key=lambda x: x['val_perplexity'])\n",
        "best_config = best_result['config']\n",
        "print(f'\\nBest model by validation perplexity:')\n",
        "print(f'  Config: n_emb={best_config[\"embedding_dim\"]}, n_hidden={best_config[\"hidden_dim\"]}')\n",
        "print(f'  Val Perplexity: {best_result[\"val_perplexity\"]:.2f}')\n",
        "print(f'  Run ID: {best_result[\"run_id\"]}')"
      ],
      "metadata": {
        "id": "results_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Get data for plotting\n",
        "configs = [f\"n_emb={r['config']['embedding_dim']}\\nn_h={r['config']['hidden_dim']}\" for r in results]\n",
        "val_ppls = [r['val_perplexity'] for r in results]\n",
        "train_ppls = [r['train_perplexity'] for r in results]\n",
        "n_params = [r['n_params'] / 1e6 for r in results]  # in millions\n",
        "\n",
        "# Plot 1: Perplexity comparison\n",
        "x = range(len(configs))\n",
        "width = 0.35\n",
        "axes[0].bar([i - width/2 for i in x], train_ppls, width, label='Train', alpha=0.8)\n",
        "axes[0].bar([i + width/2 for i in x], val_ppls, width, label='Validation', alpha=0.8)\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(configs)\n",
        "axes[0].set_ylabel('Perplexity')\n",
        "axes[0].set_title('Train vs Validation Perplexity by Configuration')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Parameters vs Perplexity\n",
        "axes[1].scatter(n_params, val_ppls, s=100, c='blue', alpha=0.7, label='Validation')\n",
        "axes[1].scatter(n_params, train_ppls, s=100, c='orange', alpha=0.7, label='Train')\n",
        "for i, txt in enumerate(configs):\n",
        "    axes[1].annotate(txt.replace('\\n', ', '), (n_params[i], val_ppls[i]), \n",
        "                     textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
        "axes[1].set_xlabel('Parameters (millions)')\n",
        "axes[1].set_ylabel('Perplexity')\n",
        "axes[1].set_title('Model Size vs Perplexity')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Text generation with best model\n",
        "def generate_sentence(model, itos, context_size=3, max_len=20, device='cuda'):\n",
        "    \"\"\"Generate a sentence using the trained model.\"\"\"\n",
        "    context = [0] * context_size  # Start with padding/block\n",
        "    out = []\n",
        "    \n",
        "    for _ in range(max_len):\n",
        "        x = torch.tensor([context], device=device)\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1).item()\n",
        "        \n",
        "        if ix == 0:  # End of sentence / Block\n",
        "            break\n",
        "        \n",
        "        context = context[1:] + [ix]\n",
        "        out.append(ix)\n",
        "    \n",
        "    return ' '.join(itos[i] for i in out)\n",
        "\n",
        "# Generate sentences with best model\n",
        "print('Generated sentences from best model:')\n",
        "print('-' * 50)\n",
        "for i in range(5):\n",
        "    sentence = generate_sentence(\n",
        "        best_result['model'], \n",
        "        best_result['itos'],\n",
        "        context_size=best_config['context_window'],\n",
        "        device=device\n",
        "    )\n",
        "    print(f'{i+1}. {sentence}')"
      ],
      "metadata": {
        "id": "text_generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: View MLflow experiment info\n",
        "print(f'\\nMLflow UI: Run `mlflow ui` in terminal at {mlflow_tracking_path}')\n",
        "print(f'Or in Colab, access via ngrok tunneling.')\n",
        "print(f'\\nExperiment runs saved to: {mlflow_tracking_path}')\n",
        "\n",
        "# List runs in the experiment\n",
        "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "if experiment:\n",
        "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "    print(f'\\nNumber of runs in experiment: {len(runs)}')\n",
        "    if len(runs) > 0:\n",
        "        print('\\nRecent runs:')\n",
        "        print(runs[['run_id', 'params.embedding_dim', 'params.hidden_dim', 'metrics.final_val_perplexity']].to_string())"
      ],
      "metadata": {
        "id": "mlflow_info"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}